{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files):\n",
    "    data, sent = [], []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as rf:\n",
    "            for line in rf:\n",
    "                if line.strip() != '':\n",
    "                    # Note: the shared corpus is already tokenized\n",
    "                    sent.append(line.strip().split('\\t'))\n",
    "                else:\n",
    "                    if len(sent) > 0:\n",
    "                        data.append(sent)\n",
    "                        sent = []\n",
    "    return data\n",
    "\n",
    "sents0 = load_data(['FB_HI_EN_CR.txt', 'TWT_HI_EN_CR.txt', 'WA_HI_EN_CR.txt'])\n",
    "sents1 = load_data(['FB_BN_EN_CR.txt', 'TWT_BN_EN_CR.txt', 'WA_BN_EN_CR.txt'])\n",
    "sents2 = load_data(['FB_TE_EN_CR.txt', 'TWT_TE_EN_CR.txt', 'WA_TE_EN_CR.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train sentences: 2104\n",
      "# Validation sentences: 526\n"
     ]
    }
   ],
   "source": [
    "# Train-Validation Set for hIn-Eng\n",
    "random.seed(7)\n",
    "random.shuffle(sents0)\n",
    "train_sents0 = sents0[:int(0.8*len(sents0))]\n",
    "valid_sents0 = sents0[int(0.8*len(sents0)):]\n",
    "print(\"# Train sentences: %d\" % (len(train_sents0)))\n",
    "print(\"# Validation sentences: %d\" % (len(valid_sents0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train sentences: 499\n",
      "# Validation sentences: 125\n"
     ]
    }
   ],
   "source": [
    "#Train - Validation Set for BN-EN\n",
    "random.seed(7)\n",
    "random.shuffle(sents1)\n",
    "train_sents1 = sents1[:int(0.8*len(sents1))]\n",
    "valid_sents1 = sents1[int(0.8*len(sents1)):]\n",
    "print(\"# Train sentences: %d\" % (len(train_sents1)))\n",
    "print(\"# Validation sentences: %d\" % (len(valid_sents1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train sentences: 1583\n",
      "# Validation sentences: 396\n"
     ]
    }
   ],
   "source": [
    "#Train - Validation Set for  TE_EN\n",
    "random.seed(7)\n",
    "random.shuffle(sents2)\n",
    "train_sents2 = sents2[:int(0.8*len(sents2))]\n",
    "valid_sents2 = sents2[int(0.8*len(sents2)):]\n",
    "print(\"# Train sentences: %d\" % (len(train_sents2)))\n",
    "print(\"# Validation sentences: %d\" % (len(valid_sents2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Bolo', 'hi', 'G_J']]\n"
     ]
    }
   ],
   "source": [
    "print(train_sents0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, k):\n",
    "    word = sent[k][0]\n",
    "    features = [\n",
    "        'token=%s' % (word)\n",
    "    ]\n",
    "    # extracting n-grams, for n=1 to 5\n",
    "    for i in range(1,6):\n",
    "        # if the value of n is greater than the word length, we exit the loop\n",
    "        if i > len(word):\n",
    "            break\n",
    "        character_features = [word[j:j+i] for j in range(len(word)-i+1)]\n",
    "        features.extend([\n",
    "            # is count of individual n-grams important? is the order important?\n",
    "            \"char-%d-gram=%s\" % (i, ' '.join(list(set(character_features))))\n",
    "        ])\n",
    "    if k == 0:\n",
    "        # first word in the sentence\n",
    "        features.append('BOS')\n",
    "    else:\n",
    "        features.extend([\n",
    "            \"-1:word=%s\" % (sent[k-1][0])\n",
    "        ])\n",
    "    if i == len(sent):\n",
    "        # last word in the sentence         \n",
    "        features.append('EOS')\n",
    " \n",
    "    return features\n",
    "        \n",
    "def sent2features(sent):\n",
    "    # generating features for all the words/tokens in a sentence `sent`    \n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2langs(sent):\n",
    "    return [language_label for token, language_label, pos_tag in sent]\n",
    "\n",
    "def sent2pos(sent):\n",
    "    return [pos_tag for token, language_label, pos_tag in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, language_label, pos_tag in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 360 ms, sys: 8 ms, total: 368 ms\n",
      "Wall time: 365 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train0 = [sent2features(sent) for sent in train_sents0]\n",
    "y_train0 = [sent2langs(sent) for sent in train_sents0]\n",
    "# for training a pos-tagging system\n",
    "# y_train = [sent2pos(sent) for sent in train_sents]\n",
    "\n",
    "X_test0 = [sent2features(sent) for sent in valid_sents0]\n",
    "y_test0 = [sent2langs(sent) for sent in valid_sents0]\n",
    "# y_test = [sent2pos(sent) for sent in valid_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 244 ms, sys: 4 ms, total: 248 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train1 = [sent2features(sent) for sent in train_sents1]\n",
    "y_train1 = [sent2langs(sent) for sent in train_sents1]\n",
    "# for training a pos-tagging system\n",
    "# y_train = [sent2pos(sent) for sent in train_sents]\n",
    "\n",
    "X_test1 = [sent2features(sent) for sent in valid_sents1]\n",
    "y_test1 = [sent2langs(sent) for sent in valid_sents1]\n",
    "# y_test = [sent2pos(sent) for sent in valid_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 312 ms, sys: 8 ms, total: 320 ms\n",
      "Wall time: 321 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train2 = [sent2features(sent) for sent in train_sents2]\n",
    "y_train2 = [sent2langs(sent) for sent in train_sents2]\n",
    "# for training a pos-tagging system\n",
    "# y_train = [sent2pos(sent) for sent in train_sents]\n",
    "\n",
    "X_test2 = [sent2features(sent) for sent in valid_sents2]\n",
    "y_test2 = [sent2langs(sent) for sent in valid_sents2]\n",
    "# y_test = [sent2pos(sent) for sent in valid_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['token=Bolo', 'char-1-gram=B o l', 'char-2-gram=lo Bo ol', 'char-3-gram=Bol olo', 'char-4-gram=Bolo', 'BOS']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 4 ms, total: 268 ms\n",
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer0 = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq0, yseq0 in zip(X_train0, y_train0):\n",
    "    trainer0.append(xseq0, yseq0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72 ms, sys: 8 ms, total: 80 ms\n",
      "Wall time: 78.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer1 = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq1, yseq1 in zip(X_train1, y_train1):\n",
    "    trainer1.append(xseq1, yseq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 184 ms, sys: 8 ms, total: 192 ms\n",
      "Wall time: 192 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer2 = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq2, yseq2 in zip(X_train2, y_train2):\n",
    "    trainer2.append(xseq2, yseq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer0.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.9 s, sys: 88 ms, total: 6.99 s\n",
      "Wall time: 6.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer0.train('icon2014_lid.crfsuite')\n",
    "\n",
    "trainer1.train('icon2014_lid.crfsuite')\n",
    "\n",
    "\n",
    "trainer2.train('icon2014_lid.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7fc713e957f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('icon2014_lid.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@KumarVkd @vijaystambh grow up ... manavhit ki soch bhai hindu hit aur muslim hit dono aapne aap ho jaygi ...\n",
      "\n",
      "Predicted: univ univ univ univ univ univ univ univ univ univ en te en en te univ univ univ univ univ\n",
      "Correct:   univ univ en en univ hi hi hi hi hi en hi hi en hi hi hi hi hi univ\n"
     ]
    }
   ],
   "source": [
    "example_sent0 = valid_sents0[10]\n",
    "print(' '.join(sent2tokens(example_sent0)), end='\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent0))))\n",
    "print(\"Correct:  \", ' '.join(sent2langs(example_sent0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@jeet30 2002 theke dekha amar swapno ta puron hoyechilo 2013 e aajker dine ! Sarajibon e din ta spcl hoye thakbe amar kache ! ðŸ’—\n",
      "\n",
      "Predicted: univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ univ\n",
      "Correct:   univ en bn bn bn bn bn bn bn en bn bn bn univ bn bn bn bn en bn bn bn bn univ undef\n"
     ]
    }
   ],
   "source": [
    "example_sent1 = valid_sents1[10]\n",
    "print(' '.join(sent2tokens(example_sent1)), end='\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent1))))\n",
    "print(\"Correct:  \", ' '.join(sent2langs(example_sent1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarainodu release eppudu\n",
      "\n",
      "Predicted: univ en te\n",
      "Correct:   te en te\n"
     ]
    }
   ],
   "source": [
    "example_sent2 = valid_sents2[10]\n",
    "print(' '.join(sent2tokens(example_sent2)), end='\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent2))))\n",
    "print(\"Correct:  \", ' '.join(sent2langs(example_sent2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
